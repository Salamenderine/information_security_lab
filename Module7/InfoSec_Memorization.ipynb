{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e57abcaab7c2492fb7aeec50fa3767c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_01d21e2393684aac82f2e72e875267aa",
              "IPY_MODEL_c3c30e0768434cb6a187c4c47bdfb872",
              "IPY_MODEL_14b4fe32b87e494286e5d589d86b6d47"
            ],
            "layout": "IPY_MODEL_62ea0cf9d38a4e358b4adda525edbbcf"
          }
        },
        "01d21e2393684aac82f2e72e875267aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4ddbe7b47fd34b8883f6644fb81a0f70",
            "placeholder": "​",
            "style": "IPY_MODEL_47b07ae97c82417d897c0d2bd4e19d18",
            "value": " 99%"
          }
        },
        "c3c30e0768434cb6a187c4c47bdfb872": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e54a1bf0713a457aab6346a634c860de",
            "max": 100000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4be3e4c35b2b4a3a85bb0b24986053ec",
            "value": 98613
          }
        },
        "14b4fe32b87e494286e5d589d86b6d47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_001584de0a454497b884ac0a2597d0e4",
            "placeholder": "​",
            "style": "IPY_MODEL_99c74b0bba7e4bb1916255661b40e2c8",
            "value": " 98613/100000 [03:21&lt;00:02, 478.77it/s]"
          }
        },
        "62ea0cf9d38a4e358b4adda525edbbcf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4ddbe7b47fd34b8883f6644fb81a0f70": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "47b07ae97c82417d897c0d2bd4e19d18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e54a1bf0713a457aab6346a634c860de": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4be3e4c35b2b4a3a85bb0b24986053ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "001584de0a454497b884ac0a2597d0e4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "99c74b0bba7e4bb1916255661b40e2c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Setup"
      ],
      "metadata": {
        "id": "-HsHGPWZBFtS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  # mount your google drive to get permanent storage for your results\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "\n",
        "  RESULTS_PATH = \"/content/drive/MyDrive/infoseclab_ML/results\"\n",
        "except ModuleNotFoundError:\n",
        "  RESULTS_PATH = \"results\"\n",
        "\n",
        "!mkdir -p {RESULTS_PATH}"
      ],
      "metadata": {
        "id": "QHH8AofBE9Ic",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c65628c-cb9a-44a6-bb48-54052e59c0ff"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "# Download the lab files\n",
        "![ ! -d 'infoseclab' ] && git clone https://github.com/ethz-privsec/infoseclab.git\n",
        "%cd infoseclab\n",
        "!git pull https://github.com/ethz-privsec/infoseclab.git\n",
        "%cd ..\n",
        "if \"infoseclab\" not in sys.path:\n",
        "  sys.path.append(\"infoseclab\")"
      ],
      "metadata": {
        "id": "qkfrTYZ7BHBX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd581cca-a520-4f90-a507-cd3724df374b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'infoseclab'...\n",
            "remote: Enumerating objects: 321, done.\u001b[K\n",
            "remote: Counting objects: 100% (40/40), done.\u001b[K\n",
            "remote: Compressing objects: 100% (29/29), done.\u001b[K\n",
            "remote: Total 321 (delta 13), reused 31 (delta 10), pack-reused 281\u001b[K\n",
            "Receiving objects: 100% (321/321), 64.87 MiB | 33.40 MiB/s, done.\n",
            "Resolving deltas: 100% (139/139), done.\n",
            "/content/infoseclab\n",
            "From https://github.com/ethz-privsec/infoseclab\n",
            " * branch            HEAD       -> FETCH_HEAD\n",
            "Already up to date.\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "3UFYO94QBIKz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import infoseclab\n",
        "from infoseclab import extraction, Vocab, PREFIX\n",
        "\n",
        "from zipfile import ZipFile\n",
        "import numpy as np\n",
        "import os\n",
        "import json\n",
        "\n",
        "device = \"cuda\"\n",
        "\n",
        "# we won't need gradients here so let's disable them to make things faster\n",
        "torch.set_grad_enabled(False)\n",
        "\n",
        "# utilities for loading & saving results\n",
        "def read_results():\n",
        "  with open(os.path.join(RESULTS_PATH, \"extraction.json\"), \"r\") as f:\n",
        "    res = json.load(f)\n",
        "  return res\n",
        "\n",
        "\n",
        "def write_results(res):\n",
        "  assert len(res) == 4\n",
        "  assert type(res) == dict\n",
        "  with open(os.path.join(RESULTS_PATH, \"extraction.json\"), \"w\") as f:\n",
        "    res = json.dump(res, f)\n",
        "\n",
        "\n",
        "def print_results(res):\n",
        "  for key, value in res.items():\n",
        "    print(f\"{key.replace('_', ' ')}: {repr(value)}\")"
      ],
      "metadata": {
        "id": "qN2qQU8dBMG7"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Create file to save results"
      ],
      "metadata": {
        "id": "0xYlh_fn7ETQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  res = read_results()\n",
        "  assert len(res) == 4\n",
        "  assert type(res) == dict\n",
        "except FileNotFoundError:\n",
        "  res = {\n",
        "      \"main_character\": None,\n",
        "      \"greedy_guess\": None,\n",
        "      \"greedy_numeric_guess\": None,\n",
        "      \"exact_guess\": None\n",
        "  }\n",
        "  write_results(res)\n",
        "\n",
        "print_results(res)"
      ],
      "metadata": {
        "id": "AmNr00RV7D4z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4cc605ec-850f-4410-ebb1-b6c46a584c27"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "main character: 'Sherlock Holmes'\n",
            "greedy guess: '3\\n an'\n",
            "greedy numeric guess: '39731'\n",
            "exact guess: '35192'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1.&nbsp;Freeform generation\n",
        "\n",
        "We will be working with a simple *character-level* language model.\n",
        "\n",
        "This is a model that takes as input a sentence (e.g., \"my name is \") and outputs a distribution over the next character in the sentence. We can then generate a character (e.g., \"F\") by sampling from this distribution. By applying the model recursively to its own output we can generate text character by character: \"my name is Florian\".\n",
        "\n",
        "Technically, the langauge model doesn't operate on `characters` but on `tokens` (numbers). The characters in the model's \"vocabulary\" are sorted, and can thus be referenced by an integer. The i-th value in the langauge model's output corresponds to the probability assigned to the i-th character in the vocabulary.\n",
        "\n",
        "You can find the full vocabulary (i.e., all characters that the language model can produce) in `infoseclab.extraction.Vocab`.\n",
        "This class has two utility dictionaries, `char_to_ix` and `ix_to_char` for converting from a character to its index (its token) and vice-versa:\n",
        "\n",
        "```\n",
        "Vocab.char_to_ix['a'] -> 54\n",
        "Vocab.ix_to_char[54] -> 'a'\n",
        "```"
      ],
      "metadata": {
        "id": "Y1PXWtQ-BnGm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load a simple character-level language model\n",
        "lm = extraction.load_lm(\"infoseclab/data/secret_model.pth\", device=device)"
      ],
      "metadata": {
        "id": "3z_zuKMQ37V1"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# example of how to generate text from the language model\n",
        "extraction.generate(lm, \"my name is\", length=100)"
      ],
      "metadata": {
        "id": "qT7c-NI57MSX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5be76bbd-e6dd-4a6e-c6dc-b1f0aa63545d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'my name is\\n in the holding of our passciable public one.\"\\n\\n \"What do you make if I a deep in the gipsy?\"\\n\\n \"To'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**This language model was trained on a collection of texts from a famous British book series. \n",
        "Your first goal is to figure out which books.**\n",
        "\n",
        "**Your guess should be in the form `\"Firstname Lastname\"` of the book series' main character.\n",
        "For example, if you guessed that the book series is Harry Potter, then your guess would be `\"Harry Potter\"`.**\n",
        "\n",
        "Note: the code immediately below doesn't check for correctness! It just checks that you've made a guess."
      ],
      "metadata": {
        "id": "zKL8VWwn4gVW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "guess = \"Sherlock Holmes\"\n",
        "res = read_results()\n",
        "res['main_character'] = guess\n",
        "write_results(res)\n",
        "print_results(res)"
      ],
      "metadata": {
        "id": "cXRYQ3B6Bpti",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c5cd6b6-6dc9-469d-96d2-bcb8f69ca911"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "main character: 'Sherlock Holmes'\n",
            "greedy guess: '3\\n an'\n",
            "greedy numeric guess: '39731'\n",
            "exact guess: '35192'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2.&nbsp;Secret extraction\n",
        "\n",
        "Unfortunately, the training data from this language model also contained the sentence `\"Florian's password is XXXXX\"`. (the real password is blanked out, your goal is to recover it!)\n",
        "\n",
        "The model might have *memorized* the correct password, and your goal will be to recover it.\n",
        "\n",
        "For this, you know the *prefix*: `\"Florian's password is \"`\n",
        "(you can find this stored under `infoseclab.extraction.PREFIX`).\n",
        "\n",
        "You also know that Florian's password is exactly 5 characters long (so that it it easier to memorize, *obviously*)."
      ],
      "metadata": {
        "id": "K3c1YfON5bx9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2.1&nbsp; Greedy secret extraction\n",
        "\n",
        "You will first attempt to extract the secret password *greedily*, simply by sampling the **5 most likely characters**, one-by-one, from the language model, starting from the known `PREFIX`.\n",
        "\n",
        "You can use the `extraction.generate` method as inspiration for this.\n",
        "\n",
        "*Note that `extraction.generate` does <b>not</b> sample greedily from the model. Rather, it samples a character at random according to the probability distribution predicted by the model.*"
      ],
      "metadata": {
        "id": "XLR_DCUHDt5l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_greedy(lm, prompt, length=5):\n",
        "    generated_text = \"\"\n",
        "    hidden_state = None\n",
        "\n",
        "    # tokenize the prompt\n",
        "    input_seq = [Vocab.char_to_ix[ch] for ch in prompt]\n",
        "    # tensor of dimension (N,) where N is the number of characters in the prompt\n",
        "    input_seq = torch.tensor(input_seq).to(lm.device)\n",
        "\n",
        "    for i in range(length):\n",
        "        # forward pass through the model\n",
        "        # output is a tensor of dimension (N, vocab_size)\n",
        "        output, hidden_state = lm.forward(input_seq, hidden_state)\n",
        "\n",
        "        # get a distribution over the next character\n",
        "        # probas is of dimension (vocab_size,)\n",
        "        probas = F.softmax(output[-1], dim=0)\n",
        "\n",
        "        # sample a character according to the predicted distribution\n",
        "        index = torch.argmax(probas)\n",
        "        generated_text += Vocab.ix_to_char[index.item()]\n",
        "\n",
        "        # to continue the generation, we simply evaluate\n",
        "        # the model on the last predicted character,\n",
        "        # and the current state\n",
        "        input_seq = torch.tensor([index.item()]).to(lm.device)\n",
        "\n",
        "    return generated_text\n",
        "\n",
        "guess_greedy = generate_greedy(lm, PREFIX, length=5)\n",
        "print(\"greedy:\", PREFIX + repr(guess_greedy))\n",
        "\n",
        "res = read_results()\n",
        "res['greedy_guess'] = guess_greedy\n",
        "write_results(res)\n",
        "print_results(res)"
      ],
      "metadata": {
        "id": "2tq-2nO8D0Z9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea88543f-50ce-4c63-e7b6-dec52b5e43e8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "greedy: Florian's password is '3\\n an'\n",
            "main character: 'Sherlock Holmes'\n",
            "greedy guess: '3\\n an'\n",
            "greedy numeric guess: '39731'\n",
            "exact guess: '35192'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2.2&nbsp;Greedy numeric secret extraction\n",
        "\n",
        "Your greedy extraction likely generated some giberish! (but hey, a password might genuinely look like that).\n",
        "\n",
        "You are now given some extra information: **Florian's password only contains numbers!** (he's not very good at security).\n",
        "\n",
        "Modify your greedy sampling mechanism to repeatedly sample the 5 most likely *numbers*, one-by-one, starting from the known `PREFIX`."
      ],
      "metadata": {
        "id": "wCtm2C2L5jep"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_greedy_numeric(lm, prompt, length=5):\n",
        "    generated_text = \"\"\n",
        "    hidden_state = None\n",
        "\n",
        "    # tokenize the prompt\n",
        "    input_seq = [Vocab.char_to_ix[ch] for ch in prompt]\n",
        "    # tensor of dimension (N,) where N is the number of characters in the prompt\n",
        "    input_seq = torch.tensor(input_seq).to(lm.device)\n",
        "\n",
        "    for i in range(length):\n",
        "        # forward pass through the model\n",
        "        # output is a tensor of dimension (N, vocab_size)\n",
        "        output, hidden_state = lm.forward(input_seq, hidden_state)\n",
        "\n",
        "        # get a distribution over the next character\n",
        "        # probas is of dimension (vocab_size,)\n",
        "        probas = F.softmax(output[-1], dim=0)\n",
        "        probas[:Vocab.char_to_ix['0'] ] = 0\n",
        "        probas[Vocab.char_to_ix['9']+1:] = 0\n",
        "\n",
        "        # sample a character according to the predicted distribution\n",
        "        index = torch.argmax(probas)\n",
        "        generated_text += Vocab.ix_to_char[index.item()]\n",
        "\n",
        "        # to continue the generation, we simply evaluate\n",
        "        # the model on the last predicted character,\n",
        "        # and the current state\n",
        "        input_seq = [Vocab.char_to_ix[ch] for ch in prompt + generated_text]\n",
        "        input_seq = torch.tensor(input_seq).to(lm.device)\n",
        "\n",
        "    return generated_text\n",
        "\n",
        "guess_greedy_numeric = generate_greedy_numeric(lm, PREFIX, length=5)\n",
        "print(\"greedy (numeric):\", PREFIX + repr(guess_greedy_numeric))\n",
        "\n",
        "# res = read_results()\n",
        "# res['greedy_numeric_guess'] = guess_greedy_numeric\n",
        "# write_results(res)\n",
        "# print_results(res)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wdo4SmxzpkmU",
        "outputId": "b0512622-3236-4f78-9c6f-4db45716eb56"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "greedy (numeric): Florian's password is '39731'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_greedy_numeric(lm, prompt, length=5):\n",
        "    generated_text = \"\"\n",
        "    hidden_state = None\n",
        "\n",
        "    # tokenize the prompt\n",
        "    input_seq = [Vocab.char_to_ix[ch] for ch in prompt]\n",
        "    # tensor of dimension (N,) where N is the number of characters in the prompt\n",
        "    input_seq = torch.tensor(input_seq).to(lm.device)\n",
        "\n",
        "    for i in range(length):\n",
        "        # forward pass through the model\n",
        "        # output is a tensor of dimension (N, vocab_size)\n",
        "        output, hidden_state = lm.forward(input_seq, hidden_state)\n",
        "\n",
        "        # get a distribution over the next character\n",
        "        # probas is of dimension (vocab_size,)\n",
        "        probas = F.softmax(output[-1], dim=0)\n",
        "        probas[:Vocab.char_to_ix['0'] ] = 0\n",
        "        probas[Vocab.char_to_ix['9']+1:] = 0\n",
        "\n",
        "        # sample a character according to the predicted distribution\n",
        "        index = torch.argmax(probas)\n",
        "        generated_text += Vocab.ix_to_char[index.item()]\n",
        "\n",
        "        # to continue the generation, we simply evaluate\n",
        "        # the model on the last predicted character,\n",
        "        # and the current state\n",
        "        input_seq = torch.tensor([index.item()]).to(lm.device)\n",
        "\n",
        "    return generated_text\n",
        "\n",
        "guess_greedy_numeric = generate_greedy_numeric(lm, PREFIX, length=5)\n",
        "print(\"greedy (numeric):\", PREFIX + repr(guess_greedy_numeric))\n",
        "\n",
        "res = read_results()\n",
        "res['greedy_numeric_guess'] = guess_greedy_numeric\n",
        "write_results(res)\n",
        "print_results(res)"
      ],
      "metadata": {
        "id": "6Pbzx4dSa1sy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebd54286-e527-4fe6-d32f-77e37a884d23"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "greedy (numeric): Florian's password is '39731'\n",
            "main character: 'Sherlock Holmes'\n",
            "greedy guess: '3\\n an'\n",
            "greedy numeric guess: '39731'\n",
            "exact guess: '35192'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2.3&nbsp;Exact numeric secret extraction\n",
        "\n",
        "Spoiler alert: the secret you found using greedy sampling is *not* Florian's password.\n",
        "\n",
        "As it turns out, sampling greedily from the model is not guaranteed to find the *sequence* of characters that is most likely according to the model's probability distribution.\n",
        "\n",
        "To illustrate, below you can compare the loss from your greedy guess, and a different (also incorrect) guess.</br>\n",
        "The guess `\"36175\"` has lower loss!"
      ],
      "metadata": {
        "id": "16tSQO1RHBxB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(guess_greedy_numeric, extraction.get_loss(lm, PREFIX + guess_greedy_numeric))\n",
        "print(\"36175\", extraction.get_loss(lm, PREFIX + \"36175\"))"
      ],
      "metadata": {
        "id": "J5xuvMF7HFg9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42689e0c-4d52-4acc-8a5b-9c94b9aeb5ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Florian's password is 39731 tensor(1.5968, device='cuda:0')\n",
            "36175 tensor(0.8980, device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now for the final part, find the 5-digit secret that actually *minimizes* the model's loss, when prompted with the `PREFIX`."
      ],
      "metadata": {
        "id": "IkmUuKQWbaVm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"35192\", extraction.get_loss(lm, PREFIX + \"35192\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_RWg-Mw0irNz",
        "outputId": "07d29cef-1b11-4f3a-cc98-cd071063ceeb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "35192 tensor(0.5321, device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.notebook import tqdm"
      ],
      "metadata": {
        "id": "ca5iK2tMg45R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_exact(lm, prompt, length=5):\n",
        "    best_guess = \"00000\"\n",
        "    best_loss = extraction.get_loss(lm, PREFIX + best_guess)\n",
        "\n",
        "    for i in tqdm(range(100000)):\n",
        "        guess = str(i).zfill(5)\n",
        "        loss = extraction.get_loss(lm, PREFIX + guess)\n",
        "        if loss < best_loss:\n",
        "            best_loss = loss\n",
        "            best_guess = guess\n",
        "    return best_guess\n",
        "\n",
        "guess_exact = generate_exact(lm, PREFIX, length=5)\n",
        "print(\"\\nexact:\", PREFIX + repr(guess_exact))\n",
        "\n",
        "res = read_results()\n",
        "res['exact_guess'] = guess_exact\n",
        "write_results(res)\n",
        "print_results(res)"
      ],
      "metadata": {
        "id": "CjLjFgyTIzgP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "e57abcaab7c2492fb7aeec50fa3767c9",
            "01d21e2393684aac82f2e72e875267aa",
            "c3c30e0768434cb6a187c4c47bdfb872",
            "14b4fe32b87e494286e5d589d86b6d47",
            "62ea0cf9d38a4e358b4adda525edbbcf",
            "4ddbe7b47fd34b8883f6644fb81a0f70",
            "47b07ae97c82417d897c0d2bd4e19d18",
            "e54a1bf0713a457aab6346a634c860de",
            "4be3e4c35b2b4a3a85bb0b24986053ec",
            "001584de0a454497b884ac0a2597d0e4",
            "99c74b0bba7e4bb1916255661b40e2c8"
          ]
        },
        "outputId": "9d4cde29-4304-4660-ca36-882aea090852"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/100000 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e57abcaab7c2492fb7aeec50fa3767c9"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create submission file (**upload `results.zip` to moodle**) \n"
      ],
      "metadata": {
        "id": "fNMIfOoL_dOt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -j -r \"{RESULTS_PATH}/results.zip\" {RESULTS_PATH} --exclude \"*x_adv_untargeted.npy\""
      ],
      "metadata": {
        "id": "S0N1Uv1Y_cLk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "807fdae7-71e2-427b-8b3c-5bf7935e75df"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "updating: x_adv_targeted.npy (deflated 10%)\n",
            "updating: x_adv_detect.npy (deflated 10%)\n",
            "updating: x_adv_random.npy (deflated 12%)\n",
            "updating: extraction.json (deflated 25%)\n",
            "updating: x_adv_jpeg.npy (deflated 10%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with ZipFile(f\"{RESULTS_PATH}/results.zip\", 'r') as zip:\n",
        "    res = json.load(zip.open(\"extraction.json\"))\n",
        "    print_results(res)"
      ],
      "metadata": {
        "id": "VSPUajuP_zcd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd2c13b6-efb8-4e99-eb17-d9fa78b6da2e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "main character: 'Sherlock Holmes'\n",
            "greedy guess: '3\\n an'\n",
            "greedy numeric guess: '39731'\n",
            "exact guess: '35192'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fOemEsbgqkPm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S1dHY8EwiyHX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}